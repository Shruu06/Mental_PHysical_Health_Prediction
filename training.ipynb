{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying all operation on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "\n",
    "# import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# label encoding\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Evaluation matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# importing pickle for saving model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training=pd.read_csv(r\"data\\training.csv\",index_col=0,header=0)\n",
    "print(training.shape)\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.drop([\"Timestamp\",\"Country\",\"self_employed\",\"state\",\"comments\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in training.columns:\n",
    "    if training[i].dtypes!=\"int64\":\n",
    "        print(i,\":\")\n",
    "        print(training[i].value_counts())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in training.columns:\n",
    "    if training[i].dtypes!=\"int64\":\n",
    "        print(i,\":\")\n",
    "        print(training[i].unique())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=training.replace([\"Male\",\"male\",\"M\",\"m\",\"Make\",\"Man\",\"Cis Male\",\"Malr\",\"something kinda male?\",\n",
    "                           \"Guy (-ish) ^_^\",\"maile\",\"Malr\",\"male leaning androgynous\",\"Male (CIS)\",\"Male-ish\",\n",
    "                           \"Mal\",\"cis male\",\"Mail\",\"msle\",\"Male \"], \"Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=training.replace([\"Female\",\"female\",\"Trans-female\",\"Cis Female\",\"F\",\"Woman\",\"f\",\"queer/she/they\",\n",
    "                           \"Femake\",\"woman\",\"Genderqueer\",\"Female  leaning androgynous\",\"cis-female/femme\",\"Trans woman\",\n",
    "                           \"Female (trans)\",\"queer\",\"Female (cis)\",\"Female \"] ,\"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=training.replace([\"non-binary\",\"Nah\",\"All\",\"Enby\",\"fluid\",\"Androgyne\",\"Agender\",\"Neuter\",\n",
    "                           \"A little about you\"] ,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in training.columns:\n",
    "    if training[i].dtypes!=\"int64\":\n",
    "        print(i,\":\")\n",
    "        print(training[i].unique())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"Gender\"].fillna(training[\"Gender\"].mode()[0],inplace=True)\n",
    "# training[\"self_employed\"].fillna(training[\"self_employed\"].mode()[0],inplace=True)\n",
    "training[\"work_interfere\"].fillna(training[\"work_interfere\"].mode()[0],inplace=True)\n",
    "# training[\"state\"].fillna(training[\"state\"].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For preprocessing the data\n",
    "le=preprocessing.LabelEncoder()\n",
    "\n",
    "colname=['Gender', 'family_history', 'work_interfere',\n",
    "       'no_employees', 'remote_work', 'tech_company', 'benefits',\n",
    "       'care_options', 'wellness_program', 'seek_help', 'anonymity', 'leave',\n",
    "       'mental_health_consequence', 'phys_health_consequence', 'coworkers',\n",
    "       'supervisor', 'mental_health_interview', 'phys_health_interview',\n",
    "       'mental_vs_physical', 'obs_consequence', 'treatment']\n",
    "\n",
    "for x in colname:\n",
    "    training[x]=le.fit_transform(training[x])\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print('Feature', x)\n",
    "    print('mapping', le_name_mapping)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.boxplot(column=\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training.Age.min())\n",
    "print(training.Age.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for value in colname:\n",
    "q1 = training['Age'].quantile(0.25) #first quartile value\n",
    "q3 = training['Age'].quantile(0.75) # third quartile value\n",
    "iqr = q3-q1 #Interquartile range\n",
    "low  = q1-1.5*iqr #acceptable range\n",
    "high = q3+1.5*iqr #acceptable range\n",
    "\n",
    "training_include = training.loc[(training['Age'] >= low) & \\\n",
    "                                (training['Age'] <= high)] # meeting the acceptable range\n",
    "training_exclude = training.loc[(training['Age'] < low) | (training['Age'] > high)] #not meeting the acceptable range\n",
    "\n",
    "print(training_include.shape)\n",
    "print(training_exclude.shape)\n",
    "\n",
    "print(low)\n",
    "\n",
    "Age_mean=int(training_include.Age.mean()) #finding the mean of the acceptable range\n",
    "print(Age_mean)\n",
    "\n",
    "#imputing outlier values with mean value\n",
    "training_exclude.Age=Age_mean\n",
    "\n",
    "#getting back the original shape of df\n",
    "training_rev=pd.concat([training_include,training_exclude],axis=0) #concatenating both dfs to get \n",
    "#the original shape\n",
    "print(training_rev.shape)\n",
    "\n",
    "training_rev.boxplot(column=[\"Age\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_rev.values[:,0:-1]\n",
    "Y = training_rev.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "\n",
    "X = scaler.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=classifier.predict(X_test)\n",
    "print(list(zip(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm=confusion_matrix(Y_test,Y_pred)\n",
    "print(cfm)\n",
    "\n",
    "print(\"Classification report: \")\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "acc=accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy of the model: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_tune=LogisticRegression()\n",
    "\n",
    "classifier_tune.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=classifier_tune.predict(X_test)\n",
    "\n",
    "print(list(zip(Y_test,Y_pred)))\n",
    "print()\n",
    "print(list(zip(training.columns[:-1],classifier.coef_.ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm=confusion_matrix(Y_test,Y_pred)\n",
    "print(cfm)\n",
    "\n",
    "print(\"Classification report: \")\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "acc=accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy of the model: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities\n",
    "y_pred_prob = classifier_tune.predict_proba(X_test)\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in np.arange(0.4,0.61,0.01):\n",
    "    predict_mine = np.where(y_pred_prob[:,1] > a, 1, 0)\n",
    "    cfm=confusion_matrix(Y_test, predict_mine)\n",
    "    total_err=cfm[0,1]+cfm[1,0]\n",
    "    print(\"Errors at threshold \", a, \":\",total_err, \" , type 2 error :\", \n",
    "          cfm[1,0],\" , type 1 error:\", cfm[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, z = metrics.roc_curve(Y_test, y_pred_prob[:,1])\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr,tpr, 'b', label = auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_SGD = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    random_state=10,\n",
    "    alpha=0.01,\n",
    "    max_iter=1000,\n",
    "    shuffle=True,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=3,\n",
    ")\n",
    "\n",
    "# Fit the classifier to your training data\n",
    "classifier_SGD.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "Y_pred = classifier_SGD.predict(X_test)\n",
    "\n",
    "# Print coefficients and intercept\n",
    "print(list(zip(training.columns[:-1], classifier_SGD.coef_.ravel())))\n",
    "print(classifier_SGD.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm=confusion_matrix(Y_test,Y_pred)\n",
    "print(cfm)\n",
    "\n",
    "print(\"Classification report: \")\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "acc=accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy of the model: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_SGD_tune = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    random_state=10,\n",
    "    alpha=0.01,\n",
    "    max_iter=1000,\n",
    "    shuffle=True,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=7,\n",
    ")\n",
    "\n",
    "classifier_SGD_tune.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = classifier_SGD_tune.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm=confusion_matrix(Y_test,Y_pred)\n",
    "print(cfm)\n",
    "\n",
    "acc=accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy of the model: \",acc)\n",
    "\n",
    "print(\"Classification report: \")\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities\n",
    "y_pred_prob = classifier_SGD_tune.predict_proba(X_test)\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in np.arange(0.3,0.61,0.01):\n",
    "    predict_mine = np.where(y_pred_prob[:,1] > a, 1, 0)\n",
    "    cfm=confusion_matrix(Y_test, predict_mine)\n",
    "    total_err=cfm[0,1]+cfm[1,0]\n",
    "    print(\"Errors at threshold \", a, \":\",total_err, \" , type 2 error :\", \n",
    "          cfm[1,0],\" , type 1 error:\", cfm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class=[]\n",
    "for value in y_pred_prob[:,1]:\n",
    "    if value > 0.45:\n",
    "        y_pred_class.append(1)\n",
    "    else:\n",
    "        y_pred_class.append(0)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm=confusion_matrix(Y_test,y_pred_class)\n",
    "print(cfm)\n",
    "acc=accuracy_score(Y_test, y_pred_class)\n",
    "print(\"Accuracy of the model: \",acc)\n",
    "print(classification_report(Y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN=KNeighborsClassifier(n_neighbors=int(np.sqrt(len(X_train))),metric='euclidean')\n",
    "\n",
    "model_KNN.fit(X_train,Y_train)\n",
    "Y_pred=model_KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(np.sqrt(len(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN_tune=KNeighborsClassifier(n_neighbors=23,metric='euclidean')\n",
    "\n",
    "model_KNN_tune.fit(X_train,Y_train)\n",
    "Y_pred=model_KNN_tune.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dict={}\n",
    "for K in range(1,60):\n",
    "    model_KNN_tune = KNeighborsClassifier(n_neighbors=K,metric=\"minkowski\")\n",
    "    model_KNN_tune.fit(X_train, Y_train) \n",
    "    Y_pred = model_KNN_tune.predict(X_test)\n",
    "    print (\"Accuracy is \", accuracy_score(Y_test,Y_pred), \"for K-Value:\",K)\n",
    "    my_dict[K]=accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in my_dict:\n",
    "    if my_dict[k]==max(my_dict.values()):\n",
    "        print(k,\":\",my_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DecisionTree=DecisionTreeClassifier(criterion=\"gini\",random_state=10,splitter=\"best\")\n",
    "\n",
    "model_DecisionTree.fit(X_train,Y_train)\n",
    "Y_pred=model_DecisionTree.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DecisionTree_tune=DecisionTreeClassifier(criterion=\"gini\",random_state=10,splitter=\"best\",\n",
    "                                         min_samples_leaf=15,max_depth=10,min_samples_split=2,\n",
    "                                         max_leaf_nodes=200)\n",
    "\n",
    "model_DecisionTree_tune.fit(X_train,Y_train)\n",
    "Y_pred=model_DecisionTree_tune.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities\n",
    "y_pred_prob = model_DecisionTree_tune.predict_proba(X_test)\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in np.arange(0.3,0.61,0.01):\n",
    "    predict_mine = np.where(y_pred_prob[:,1] > a, 1, 0)\n",
    "    cfm=confusion_matrix(Y_test, predict_mine)\n",
    "    total_err=cfm[0,1]+cfm[1,0]\n",
    "    print(\"Errors at threshold \", a, \":\",total_err, \" , type 2 error :\", \n",
    "          cfm[1,0],\" , type 1 error:\", cfm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class=[]\n",
    "for value in y_pred_prob[:,1]:\n",
    "    if value > 0.49:\n",
    "        y_pred_class.append(1)\n",
    "    else:\n",
    "        y_pred_class.append(0)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cfm=confusion_matrix(Y_test,y_pred_class)\n",
    "print(cfm)\n",
    "acc=accuracy_score(Y_test, y_pred_class)\n",
    "print(\"Accuracy of the model: \",acc)\n",
    "print(classification_report(Y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RandomForest=RandomForestClassifier(n_estimators=10, random_state=10)\n",
    "\n",
    "model_RandomForest.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=model_RandomForest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RandomForest_tune=RandomForestClassifier(n_estimators=12, random_state=10)\n",
    "\n",
    "model_RandomForest_tune.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=model_RandomForest_tune.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities\n",
    "y_pred_prob = model_RandomForest_tune.predict_proba(X_test)\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in np.arange(0.3,0.61,0.01):\n",
    "    predict_mine = np.where(y_pred_prob[:,1] > a, 1, 0)\n",
    "    cfm=confusion_matrix(Y_test, predict_mine)\n",
    "    total_err=cfm[0,1]+cfm[1,0]\n",
    "    print(\"Errors at threshold \", a, \":\",total_err, \" , type 2 error :\", \n",
    "          cfm[1,0],\" , type 1 error:\", cfm[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model=svm.SVC(kernel='rbf',C=10.0,gamma=0.001)\n",
    "\n",
    "svc_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred=svc_model.predict(X_test)\n",
    "\n",
    "print(list(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC_tune=svm.SVC(kernel=\"rbf\", gamma=0.00001, C=200)\n",
    "\n",
    "model_SVC_tune.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=model_SVC_tune.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_DecisionTree_tune = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",\n",
    "    random_state=10,\n",
    "    splitter=\"best\",\n",
    "    min_samples_leaf=15,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    max_leaf_nodes=200,\n",
    ")\n",
    "\n",
    "# Performing k-fold cross-validation\n",
    "kfold_cv = KFold(n_splits=10, random_state=10, shuffle=True)\n",
    "\n",
    "# Running the model using accuracy as the scoring metric\n",
    "kfold_cv_result = cross_val_score(\n",
    "    estimator=model_DecisionTree_tune, X=X_train, y=Y_train, cv=kfold_cv\n",
    ")\n",
    "\n",
    "# Print cross-validation results\n",
    "print(kfold_cv_result)\n",
    "\n",
    "# Find the mean accuracy\n",
    "print(kfold_cv_result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Define KFold cross-validation with shuffle=True\n",
    "kfold_cv = KFold(n_splits=15, shuffle=True, random_state=10)\n",
    "\n",
    "# Run the model using accuracy as the scoring metric\n",
    "kfold_cv_result = cross_val_score(\n",
    "    estimator=classifier, X=X_train, y=Y_train, cv=kfold_cv\n",
    ")\n",
    "\n",
    "# Print cross-validation results\n",
    "print(kfold_cv_result)\n",
    "\n",
    "# Find the mean accuracy\n",
    "print(kfold_cv_result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('log', model1))\n",
    "\n",
    "\n",
    "# model2=SGDClassifier(loss=\"log\",random_state=10,alpha=0.1,max_iter=20, shuffle=True,early_stopping=True, \n",
    "#                      n_iter_no_change=20)\n",
    "# estimators.append(('sgd', model2))\n",
    "\n",
    "\n",
    "\n",
    "model3 = DecisionTreeClassifier(criterion=\"gini\",random_state=10,splitter=\"best\",min_samples_leaf=15,max_depth=10,\n",
    "                                min_samples_split=2, max_leaf_nodes=200)\n",
    "estimators.append(('cart', model3))\n",
    "\n",
    "\n",
    "\n",
    "# model4 = svm.SVC(kernel=\"rbf\", gamma=0.00001, C=200)\n",
    "# estimators.append(('svm', model4))\n",
    "\n",
    "\n",
    "\n",
    "# model5 = KNeighborsClassifier(n_neighbors=30,metric='euclidean')\n",
    "# estimators.append(('knn', model5))\n",
    "\n",
    "\n",
    "\n",
    "model6=RandomForestClassifier(n_estimators=50, random_state=10)\n",
    "estimators.append(('rt', model6))\n",
    "\n",
    "\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "ensemble.fit(X_train,Y_train)\n",
    "Y_pred=ensemble.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying all operation on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(r\"data\\test.csv\",index_col=0,header=0)\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop([\"Timestamp\",\"Country\",\"self_employed\",\"state\",\"comments\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.columns:\n",
    "    if test[i].dtypes!=\"int64\":\n",
    "        print(i,\":\")\n",
    "        print(test[i].value_counts())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.replace([\"Male\",\"male\",\"M\",\"m\",\"Make\",\"Man\",\"Cis Male\",\"Malr\",\"something kinda male?\",\n",
    "                           \"Guy (-ish) ^_^\",\"maile\",\"Malr\",\"male leaning androgynous\",\"Male (CIS)\",\"Male-ish\",\n",
    "                           \"Mal\",\"cis male\",\"Mail\",\"msle\",\"Male \",\"Cis Man\",\n",
    "                   \"ostensibly male, unsure what that really means\"], \"Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.replace([\"Female\",\"female\",\"Trans-female\",\"Cis Female\",\"F\",\"Woman\",\"f\",\"queer/she/they\",\n",
    "                           \"Femake\",\"woman\",\"Genderqueer\",\"Female  leaning androgynous\",\"cis-female/femme\",\"Trans woman\",\n",
    "                           \"Female (trans)\",\"queer\",\"Female (cis)\",\"Female \",\"femail\"] ,\"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.replace([\"non-binary\",\"Nah\",\"All\",\"Enby\",\"fluid\",\"Androgyne\",\"Agender\",\"Neuter\",\n",
    "                           \"A little about you\",\"p\"] ,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in test.columns:\n",
    "    if test[i].dtypes!=\"int64\":\n",
    "        print(i,\":\")\n",
    "        print(test[i].unique())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Gender\"].fillna(test[\"Gender\"].mode()[0],inplace=True)\n",
    "# training[\"self_employed\"].fillna(training[\"self_employed\"].mode()[0],inplace=True)\n",
    "test[\"work_interfere\"].fillna(test[\"work_interfere\"].mode()[0],inplace=True)\n",
    "# training[\"state\"].fillna(training[\"state\"].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colname1=['Gender', 'family_history', 'work_interfere',\n",
    "       'no_employees', 'remote_work', 'tech_company', 'benefits',\n",
    "       'care_options', 'wellness_program', 'seek_help', 'anonymity', 'leave',\n",
    "       'mental_health_consequence', 'phys_health_consequence', 'coworkers',\n",
    "       'supervisor', 'mental_health_interview', 'phys_health_interview',\n",
    "       'mental_vs_physical', 'obs_consequence']\n",
    "\n",
    "for x in colname1:\n",
    "    test[x]=le.fit_transform(test[x])\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print('Feature', x)\n",
    "    print('mapping', le_name_mapping)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.boxplot(column=\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = test.values[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new  = scaler.transform(X_test_new )\n",
    "print(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DecisionTreeClassifier with desired parameters\n",
    "model_DecisionTree_tune = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",\n",
    "    random_state=10,\n",
    "    splitter=\"best\",\n",
    "    min_samples_leaf=15,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    max_leaf_nodes=200,\n",
    ")\n",
    "\n",
    "# Fit the model with training data\n",
    "model_DecisionTree_tune.fit(X_train, Y_train)\n",
    "\n",
    "# Now, you can use the predict_proba method\n",
    "Y_pred_prob = model_DecisionTree_tune.predict_proba(X_test_new)\n",
    "print(Y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class=[]\n",
    "for value in Y_pred_prob[:,1]:\n",
    "    if value > 0.49:\n",
    "        y_pred_class.append(1)\n",
    "    else:\n",
    "        y_pred_class.append(0)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing in submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(r\"data\\sample.csv\",header=0)\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"treatment\"]=y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['treatment']=df1['treatment'].replace([0], \"No\")\n",
    "df1['treatment']=df1['treatment'].replace([1], \"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r\"data\\sample.csv\",index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving model:\n",
    "pickle.dump(classifier,open(r'models\\log.pkl','wb'))\n",
    "pickle.dump(classifier_SGD,open(r'models\\sgd.pkl','wb'))\n",
    "pickle.dump(classifier_SGD_tune,open(r'models\\sgd_tune.pkl','wb'))\n",
    "pickle.dump(model_KNN,open(r'models\\knn.pkl','wb'))\n",
    "pickle.dump(model_KNN_tune,open(r'models\\knn_tune.pkl','wb'))\n",
    "pickle.dump(svc_model,open(r'models\\svc.pkl','wb'))\n",
    "pickle.dump(model_SVC_tune,open(r'models\\svc_tune.pkl','wb'))\n",
    "pickle.dump(model_DecisionTree,open(r'models\\dt.pkl','wb'))\n",
    "pickle.dump(model_DecisionTree_tune,open(r'models\\dt_tune.pkl','wb'))\n",
    "pickle.dump(model_RandomForest,open(r'models\\rt.pkl','wb'))\n",
    "pickle.dump(model_RandomForest_tune,open(r'models\\rt_tune.pkl','wb'))\n",
    "pickle.dump(ensemble,open(r'models\\ensemble.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
